# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17i9eM2ICAhwWBGeFI4xcWUxQGmhh_o2R
"""

# SOURCE CODE: https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/blob/master/Chapter04/ch4_nb2_reuse_models_from_keras_apps.ipynb
import tensorflow as tf
import os
from matplotlib import pyplot as plt
import math
import numpy as np
import pandas as pd
import cv2
import seaborn as sns
from tqdm import tqdm
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, confusion_matrix
import keras
from keras.preprocessing import image
from keras.models import Sequential, Model
from keras.layers import Dropout, MaxPooling2D, Dense, Conv2D, Activation, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import ResNet50V2
import warnings
warnings.filterwarnings("ignore")

# Choosing which GPU this notebook can access
# (useful when running multiple experiments in parallel, on different GPUs):
os.environ["CUDA_VISIBLE_DEVICES"]= "0"

# Some hyper-parameters:
input_shape = [224, 224, 3] # We will resize the input images to this shape
batch_size  = 32            # Images per batch (reduce/increase according to the machine's capability)
num_epochs  = 300           # Max number of training epochs
random_seed = 42            # Seed for some random operations, for reproducibility

#SOURCE DATA: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data
#number of chest Xrays in train folder: 1341 normal, 3875 pneumonia
#number of chest Xrays in test folder: 234 normal, 390 pneumonia

#INSTRUCTIONS UPLOADING KAGGLE DATASET TO GOOGLE COLAB: https://www.geeksforgeeks.org/how-to-import-kaggle-datasets-directly-into-google-colab/

import opendatasets as od
import pandas
od.download(
    "https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data")

# Number of classes:
num_classes = 2
class_names = ['NORMAL', 'PNEUMONIA']
# Number of images:
num_train_imgs = 1341 + 3875
num_val_imgs = 234 + 390
train_steps_per_epoch = math.ceil(num_train_imgs / batch_size)
val_steps_per_epoch   = math.ceil(num_val_imgs / batch_size)

#CODE HELP SOURCE FROM TENSORFLOW DOCUMENTATION: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory
# CODE HELP SOURCE FROM TENSORFLOW DOCUMENTATION: https://www.tensorflow.org/tutorials/load_data/images
#MAKING THE TrainING SET
#in this project we focus on using the dataset from chest-xray train which is further subdivided into 0.2-0.8 split of training and testing
#Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b).
train_ds = tf.keras.utils.image_dataset_from_directory(
  "/content/chest-xray-pneumonia/chest_xray/train/",
  labels='inferred',
  validation_split=0.2,
  subset="training",
  seed=random_seed,
  image_size=(224, 224),
  batch_size=batch_size, shuffle=True,
    )

#CODE HELP SOURCE FROM TENSORFLOW DOCUMENTATION: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory
# https://www.tensorflow.org/tutorials/load_data/images
#in this project we focus on using the dataset from chest-xray train which is further subdivided into 0.2-0.8 split of training and testing
#Then calling image_dataset_from_directory(main_directory, labels='inferred') will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b).
test_ds = tf.keras.utils.image_dataset_from_directory(
  "/content/chest-xray-pneumonia/chest_xray/train/",
  labels='inferred',
  validation_split=0.2,
  subset="validation",
  seed=random_seed,
  image_size=(224, 224),
  batch_size=batch_size, shuffle=True,
    )

#visualizing first nine images from the training set
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(15):
    ax = plt.subplot(5, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

#INITIATE RESNET
#source code: https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/blob/9a73003eff274f288d59dfb1532a5a48655bfaa3/Chapter04/ch4_nb1_implement_resnet_from_scratch.ipynb
model = tf.keras.applications.resnet50.ResNet50(
    include_top=True, weights=None,
    input_shape=input_shape, classes=num_classes)
model.summary()

#standardizing the data
#standardize RGB channels from [0,255] to [0,1]
#source: https://www.tensorflow.org/tutorials/load_data/images
normalization_layer = tf.keras.layers.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
print(np.min(first_image), np.max(first_image))

#configure dataset for performance
#prevent I/O blocking when calling data from disk
#code source: https://www.tensorflow.org/tutorials/load_data/images
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

#apply training now to the data using the model, use Adam optimizer and Sparse Categorical Cross Entropy for loss function, add some metrics of performance
model.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model.fit(
  train_ds,
  validation_data=train_ds,
  epochs=5
)